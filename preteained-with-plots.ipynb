{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":9596550,"datasetId":5853975,"databundleVersionId":9818111}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install split-folders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:18:02.686586Z","iopub.execute_input":"2025-04-24T04:18:02.686769Z","iopub.status.idle":"2025-04-24T04:18:09.102759Z","shell.execute_reply.started":"2025-04-24T04:18:02.686752Z","shell.execute_reply":"2025-04-24T04:18:09.101758Z"}},"outputs":[{"name":"stdout","text":"Collecting split-folders\n  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\nDownloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport math\nimport os\nimport numpy as np\n\n# PYTORCH\nimport torch\nfrom torch import optim, nn, utils, Tensor\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, ReduceLROnPlateau\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport splitfolders\n\n# TORCHMETRICS & TORCHVISION\nimport torchmetrics\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import Compose, Normalize, Resize, ToTensor, RandAugment\nfrom torchsummary import summary\nfrom transformers import ViTForImageClassification, ViTConfig, ViTFeatureExtractor\n\n# LIGHTNING MODULE\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, RichProgressBar, ModelCheckpoint, EarlyStopping\n\n# PLOTTING & LOGGING\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:28:50.829450Z","iopub.execute_input":"2025-04-24T04:28:50.829983Z","iopub.status.idle":"2025-04-24T04:29:14.206173Z","shell.execute_reply.started":"2025-04-24T04:28:50.829958Z","shell.execute_reply":"2025-04-24T04:29:14.205551Z"}},"outputs":[{"name":"stderr","text":"2025-04-24 04:28:56.354703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745468936.896991      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745468937.040125      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:18:29.365267Z","iopub.execute_input":"2025-04-24T04:18:29.365593Z","iopub.status.idle":"2025-04-24T04:18:29.401999Z","shell.execute_reply.started":"2025-04-24T04:18:29.365575Z","shell.execute_reply":"2025-04-24T04:18:29.401445Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"KAGGLE_INPUT_DIR = '/kaggle/input/forest/RawFireData'\nKAGGLE_WORKING_DIR = '/kaggle/working/FireData'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:18:29.402681Z","iopub.execute_input":"2025-04-24T04:18:29.402906Z","iopub.status.idle":"2025-04-24T04:18:29.419976Z","shell.execute_reply.started":"2025-04-24T04:18:29.402880Z","shell.execute_reply":"2025-04-24T04:18:29.419433Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"torch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:18:29.420737Z","iopub.execute_input":"2025-04-24T04:18:29.420953Z","iopub.status.idle":"2025-04-24T04:18:29.446669Z","shell.execute_reply.started":"2025-04-24T04:18:29.420937Z","shell.execute_reply":"2025-04-24T04:18:29.445886Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import math\nimport os\nimport numpy as np\n\n# PYTORCH\nimport torch\nfrom torch import optim, nn, utils, Tensor\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, ReduceLROnPlateau\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport splitfolders\n\n# TORCHMETRICS & TORCHVISION\nimport torchmetrics\nfrom torchvision import datasets, transforms, models\nfrom torchvision.transforms import Compose, Normalize, Resize, ToTensor, RandAugment\nfrom torchsummary import summary\n\n# TRANSFORMERS LIBRARY FOR PRETRAINED VIT\nfrom transformers import ViTForImageClassification, ViTConfig, ViTFeatureExtractor\n\n# LIGHTNING MODULE\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, RichProgressBar, ModelCheckpoint, EarlyStopping\n\n# PLOTTING & LOGGING\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nKAGGLE_INPUT_DIR = '/kaggle/input/forest/RawFireData'\nKAGGLE_WORKING_DIR = '/kaggle/working/FireData'\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nbatch_size = 32\nimg_height = 224\nimg_width = 224\nlearning_rate = 2e-4\nepochs = 10\nnum_classes = 3\n\n# Lists to store training and validation metrics\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\nos.makedirs('/kaggle/working/FireData', exist_ok=True)\n\n# Split the data\nsplitfolders.ratio(\n    '/kaggle/input/forest/RawFireData', \n    output='/kaggle/working/FireData', \n    seed=42, \n    ratio=(.8, .1, .1), \n    move=False\n)\n\nclass FireDataModule(pl.LightningDataModule):\n    def __init__(self, path=KAGGLE_WORKING_DIR, batch_size=32, img_height=224, img_width=224):\n        super().__init__()\n        self.batch_size = batch_size\n        self.img_height = img_height\n        self.img_width = img_width\n        self.PATH = path\n        self.prepare_data_per_node = False\n        self._log_hyperparams = False\n        \n    def prepare_data(self):\n        # This method is called only once and on only one GPU\n        pass\n\n    def setup(self, stage=None):\n        # Set transformations for ViT (using standard ImageNet normalization)\n        train_transform = Compose([\n            Resize((self.img_height, self.img_width)),\n            RandAugment(num_ops=2, magnitude=9),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        val_transform = Compose([\n            Resize((self.img_height, self.img_width)),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            data_dir = os.path.join(self.PATH, 'train')\n            self.train = datasets.ImageFolder(data_dir, transform=train_transform)\n            \n            data_dir = os.path.join(self.PATH, 'val')\n            self.validate = datasets.ImageFolder(data_dir, transform=val_transform)\n            \n            print(f\"Training dataset size: {len(self.train)}\")\n            print(f\"Validation dataset size: {len(self.validate)}\")\n            print(f\"Class mapping: {self.train.class_to_idx}\")\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            data_dir = os.path.join(self.PATH, 'test')\n            self.test = datasets.ImageFolder(data_dir, transform=val_transform)\n            print(f\"Test dataset size: {len(self.test)}\")\n            \n    def train_dataloader(self):\n        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=2)\n\n    def val_dataloader(self):\n        return DataLoader(self.validate, batch_size=self.batch_size, shuffle=False, num_workers=2)\n\n    def test_dataloader(self):\n        return DataLoader(self.test, batch_size=self.batch_size, shuffle=False, num_workers=2)\n\n    def predict_dataloader(self):\n        return DataLoader(self.test, batch_size=self.batch_size, shuffle=False, num_workers=2)\n\n# Custom callback to save metrics after each epoch\nclass MetricsCallback(pl.Callback):\n    def on_validation_epoch_end(self, trainer, pl_module):\n        global train_losses, train_accuracies, val_losses, val_accuracies\n        \n        # Get the metrics from the trainer\n        logs = trainer.callback_metrics\n        print(f\"Metric keys available: {logs.keys()}\")  # Add this for debugging\n        \n        # Store the metrics in our lists - make sure all metrics are captured at the same time\n        train_loss = logs.get('train_loss', None)\n        train_acc = logs.get('train_acc', None)\n        val_loss = logs.get('val_loss', None)\n        val_acc = logs.get('val_acc', None)\n        \n        # Only append if all metrics are available\n        if train_loss is not None and train_acc is not None and val_loss is not None and val_acc is not None:\n            train_losses.append(train_loss.item())\n            train_accuracies.append(train_acc.item())\n            val_losses.append(val_loss.item())\n            val_accuracies.append(val_acc.item())\n            \n            # Print for debugging\n            print(f\"Epoch metrics - Train Loss: {train_loss.item():.4f}, Train Acc: {train_acc.item():.4f}, \"\n                  f\"Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc.item():.4f}\")\n\nclass PretrainedViTForFireDetection(pl.LightningModule):\n    def __init__(\n        self,\n        num_classes=3,\n        learning_rate=2e-4,\n        pretrained_model_name=\"google/vit-base-patch16-224\",\n        fine_tune_layers=2  # Only fine-tune the last few layers\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        self.learning_rate = learning_rate\n        self.num_classes = num_classes\n        \n        # Load pretrained ViT model\n        self.model = ViTForImageClassification.from_pretrained(\n            pretrained_model_name,\n            num_labels=num_classes,\n            ignore_mismatched_sizes=True  # Important when changing the number of classes\n        )\n        \n        # Freeze most of the model parameters to prevent overfitting\n        # Only fine-tune the last few layers\n        modules = list(self.model.children())\n        for module in modules[:-fine_tune_layers]:\n            for param in module.parameters():\n                param.requires_grad = False\n                \n        # Metrics\n        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        \n    def forward(self, x):\n        return self.model(x).logits\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Log metrics\n        self.train_acc(logits, y)\n        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Log metrics\n        self.val_acc(logits, y)\n        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        self.log('val_acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n        \n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Log metrics\n        self.test_acc(logits, y)\n        self.log('test_loss', loss, prog_bar=True)\n        self.log('test_acc', self.test_acc, prog_bar=True)\n        \n        return loss\n    \n    def predict_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        preds = torch.argmax(logits, dim=1)\n        return preds\n    \n    def configure_optimizers(self):\n        # Use different learning rates for pretrained vs new layers\n        # Get parameter groups with different learning rates\n        params = [\n            # Parameters of newly constructed modules have higher learning rate\n            {'params': [p for n, p in self.named_parameters() if 'classifier' in n], 'lr': self.learning_rate},\n            # Fine-tuned parameters have lower learning rate\n            {'params': [p for n, p in self.named_parameters() if 'classifier' not in n and p.requires_grad], \n             'lr': self.learning_rate / 10}\n        ]\n        \n        optimizer = optim.AdamW(\n            params,\n            weight_decay=0.05\n        )\n        \n        # Cosine annealing learning rate scheduler\n        scheduler = CosineAnnealingLR(\n            optimizer,\n            T_max=epochs,\n            eta_min=1e-6\n        )\n        \n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\",\n                \"frequency\": 1\n            }\n        }\n\n# Function to plot training and validation metrics\ndef plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies):\n    # Print the lengths of each list for debugging\n    print(f\"Lengths - train_losses: {len(train_losses)}, train_accuracies: {len(train_accuracies)}, \"\n          f\"val_losses: {len(val_losses)}, val_accuracies: {len(val_accuracies)}\")\n    \n    # Find the minimum length to ensure all data can be plotted\n    min_length = min(len(train_losses), len(val_losses), len(train_accuracies), len(val_accuracies))\n    \n    # Truncate all lists to the minimum length\n    train_losses = train_losses[:min_length]\n    val_losses = val_losses[:min_length]\n    train_accuracies = train_accuracies[:min_length]\n    val_accuracies = val_accuracies[:min_length]\n    \n    # Now plot with equal length lists\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    epochs_range = range(1, min_length + 1)\n    plt.plot(epochs_range, train_losses, 'b-', label='Training Loss')\n    plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot accuracies\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, train_accuracies, 'b-', label='Training Accuracy')\n    plt.plot(epochs_range, val_accuracies, 'r-', label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/training_metrics.png')\n    plt.close()\n    \n    print(f\"Training metrics plot saved to: /kaggle/working/training_metrics.png\")\n    \n    # Return the paths to the saved plots\n    return '/kaggle/working/training_metrics.png'\n\ndef evaluate_model(model, data_module):\n    # Test accuracy\n    trainer = pl.Trainer(accelerator='auto')\n    results = trainer.test(model, datamodule=data_module)\n    print(f\"Test accuracy: {results[0]['test_acc']:.4f}\")\n    \n    # Get predictions for confusion matrix and visualization\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_labels = []\n    sample_images = []\n    sample_labels = []\n    sample_preds = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch_idx, (images, labels) in enumerate(test_loader):\n            images = images.to(model.device)\n            labels = labels.to(model.device)\n            \n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            # Save first batch for visualization\n            if batch_idx == 0:\n                for i in range(min(5, len(images))):\n                    sample_images.append(images[i].cpu())\n                    sample_labels.append(labels[i].item())\n                    sample_preds.append(preds[i].item())\n    \n    # Create confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    class_names = test_loader.dataset.classes\n    \n    # Generate classification report\n    report = classification_report(all_labels, all_preds, target_names=class_names)\n    print(\"Classification Report:\")\n    print(report)\n    \n    # Save classification report to file\n    with open('/kaggle/working/classification_report.txt', 'w') as f:\n        f.write(report)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    disp.plot(cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.savefig('/kaggle/working/confusion_matrix.png')\n    plt.close()\n    \n    # Plot sample predictions\n    fig, axes = plt.subplots(1, len(sample_images), figsize=(15, 3))\n    for i, (img, true_label, pred_label) in enumerate(zip(sample_images, sample_labels, sample_preds)):\n        # Denormalize image\n        img = img.permute(1, 2, 0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n        \n        axes[i].imshow(img)\n        color = 'green' if true_label == pred_label else 'red'\n        axes[i].set_title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\", color=color)\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/sample_predictions.png')\n    plt.close()\n    \n    return {\n        'accuracy': results[0]['test_acc'],\n        'confusion_matrix': '/kaggle/working/confusion_matrix.png',\n        'sample_predictions': '/kaggle/working/sample_predictions.png',\n        'classification_report': '/kaggle/working/classification_report.txt'\n    }\n\n# Create our metrics callback\nmetrics_callback = MetricsCallback()\n\n# Define callbacks\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_loss',\n    dirpath='/kaggle/working/',\n    filename='vit-fire-detection-{epoch:02d}-{val_loss:.2f}',\n    save_top_k=1,\n    mode='min'\n)\n\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    mode='min'\n)\n\nlr_monitor = LearningRateMonitor(logging_interval='epoch')\n\n# Create data module\ndata_module = FireDataModule()\ndata_module.setup()\n\n# Create model\nmodel = PretrainedViTForFireDetection(\n    num_classes=3,\n    learning_rate=2e-4,\n    fine_tune_layers=2  # Only fine-tune the last 2 layers\n)\n\n# Create trainer with callbacks\ntrainer = pl.Trainer(\n    max_epochs=epochs,\n    accelerator='auto',\n    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor, metrics_callback],\n    log_every_n_steps=10\n)\n\n# Train the model\ntrainer.fit(model, data_module)\n\n# Plot training metrics\nmetrics_plot = plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies)\n\n# Print training metrics\nprint(\"Training Metrics:\")\nfor epoch in range(min(len(train_losses), len(val_losses))):\n    print(f\"Epoch {epoch+1}: Train Loss: {train_losses[epoch]:.4f}, Train Acc: {train_accuracies[epoch]:.4f}, \"\n          f\"Val Loss: {val_losses[epoch]:.4f}, Val Acc: {val_accuracies[epoch]:.4f}\")\n\n# After training completes, evaluate the model\nresults = evaluate_model(model, data_module)\nprint(f\"Final test accuracy: {results['accuracy']:.4f}\")\nprint(f\"Confusion matrix saved to: {results['confusion_matrix']}\")\nprint(f\"Sample predictions saved to: {results['sample_predictions']}\")\nprint(f\"Classification report saved to: {results['classification_report']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T05:10:53.979787Z","iopub.execute_input":"2025-04-24T05:10:53.980569Z","iopub.status.idle":"2025-04-24T05:42:26.484204Z","shell.execute_reply.started":"2025-04-24T05:10:53.980535Z","shell.execute_reply":"2025-04-24T05:42:26.483501Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"\nCopying files: 0 files [00:00, ? files/s]\u001b[A\nCopying files: 1 files [00:05,  5.60s/ files]\u001b[A\nCopying files: 60 files [00:05, 14.84 files/s]\u001b[A\nCopying files: 116 files [00:05, 33.51 files/s]\u001b[A\nCopying files: 171 files [00:05, 57.57 files/s]\u001b[A\nCopying files: 226 files [00:06, 88.32 files/s]\u001b[A\nCopying files: 287 files [00:06, 130.93 files/s]\u001b[A\nCopying files: 341 files [00:06, 173.55 files/s]\u001b[A\nCopying files: 400 files [00:06, 227.41 files/s]\u001b[A\nCopying files: 458 files [00:06, 282.18 files/s]\u001b[A\nCopying files: 514 files [00:06, 322.90 files/s]\u001b[A\nCopying files: 575 files [00:06, 379.74 files/s]\u001b[A\nCopying files: 640 files [00:06, 439.94 files/s]\u001b[A\nCopying files: 702 files [00:06, 483.08 files/s]\u001b[A\nCopying files: 762 files [00:06, 511.96 files/s]\u001b[A\nCopying files: 822 files [00:07, 527.99 files/s]\u001b[A\nCopying files: 881 files [00:07, 533.49 files/s]\u001b[A\nCopying files: 944 files [00:07, 558.32 files/s]\u001b[A\nCopying files: 1003 files [00:07, 549.28 files/s]\u001b[A\nCopying files: 1061 files [00:07, 542.20 files/s]\u001b[A\nCopying files: 1120 files [00:07, 553.74 files/s]\u001b[A\nCopying files: 1177 files [00:07, 539.47 files/s]\u001b[A\nCopying files: 1234 files [00:07, 548.02 files/s]\u001b[A\nCopying files: 1294 files [00:07, 560.85 files/s]\u001b[A\nCopying files: 1359 files [00:07, 584.54 files/s]\u001b[A\nCopying files: 1418 files [00:08, 574.68 files/s]\u001b[A\nCopying files: 1476 files [00:08, 545.89 files/s]\u001b[A\nCopying files: 1534 files [00:08, 553.24 files/s]\u001b[A\nCopying files: 1596 files [00:08, 569.99 files/s]\u001b[A\nCopying files: 1654 files [00:08, 571.17 files/s]\u001b[A\nCopying files: 1715 files [00:08, 581.94 files/s]\u001b[A\nCopying files: 1774 files [00:08, 574.18 files/s]\u001b[A\nCopying files: 1838 files [00:08, 591.92 files/s]\u001b[A\nCopying files: 1900 files [00:08, 597.79 files/s]\u001b[A\nCopying files: 1961 files [00:09, 599.79 files/s]\u001b[A\nCopying files: 2022 files [00:09, 563.26 files/s]\u001b[A\nCopying files: 2081 files [00:09, 569.59 files/s]\u001b[A\nCopying files: 2147 files [00:09, 593.84 files/s]\u001b[A\nCopying files: 2214 files [00:09, 614.76 files/s]\u001b[A\nCopying files: 2276 files [00:09, 592.21 files/s]\u001b[A\nCopying files: 2343 files [00:09, 612.53 files/s]\u001b[A\nCopying files: 2405 files [00:09, 606.78 files/s]\u001b[A\nCopying files: 2467 files [00:09, 608.47 files/s]\u001b[A\nCopying files: 2529 files [00:15, 35.13 files/s] \u001b[A\nCopying files: 2602 files [00:15, 51.41 files/s]\u001b[A\nCopying files: 2676 files [00:15, 73.67 files/s]\u001b[A\nCopying files: 2746 files [00:15, 101.31 files/s]\u001b[A\nCopying files: 2812 files [00:15, 134.41 files/s]\u001b[A\nCopying files: 2885 files [00:16, 180.93 files/s]\u001b[A\nCopying files: 2956 files [00:16, 233.93 files/s]\u001b[A\nCopying files: 3024 files [00:16, 285.64 files/s]\u001b[A\nCopying files: 3092 files [00:16, 344.75 files/s]\u001b[A\nCopying files: 3159 files [00:16, 393.53 files/s]\u001b[A\nCopying files: 3228 files [00:16, 451.87 files/s]\u001b[A\nCopying files: 3294 files [00:16, 491.68 files/s]\u001b[A\nCopying files: 3360 files [00:16, 531.05 files/s]\u001b[A\nCopying files: 3425 files [00:16, 559.77 files/s]\u001b[A\nCopying files: 3491 files [00:17, 585.95 files/s]\u001b[A\nCopying files: 3562 files [00:17, 618.77 files/s]\u001b[A\nCopying files: 3632 files [00:17, 638.98 files/s]\u001b[A\nCopying files: 3700 files [00:17, 623.10 files/s]\u001b[A\nCopying files: 3765 files [00:17, 614.21 files/s]\u001b[A\nCopying files: 3831 files [00:17, 625.73 files/s]\u001b[A\nCopying files: 3899 files [00:17, 640.23 files/s]\u001b[A\nCopying files: 3970 files [00:17, 659.65 files/s]\u001b[A\nCopying files: 4040 files [00:17, 669.35 files/s]\u001b[A\nCopying files: 4116 files [00:17, 695.10 files/s]\u001b[A\nCopying files: 4187 files [00:18, 682.56 files/s]\u001b[A\nCopying files: 4256 files [00:18, 646.70 files/s]\u001b[A\nCopying files: 4331 files [00:18, 674.94 files/s]\u001b[A\nCopying files: 4400 files [00:18, 662.03 files/s]\u001b[A\nCopying files: 4467 files [00:18, 658.07 files/s]\u001b[A\nCopying files: 4535 files [00:18, 664.14 files/s]\u001b[A\nCopying files: 4611 files [00:18, 690.47 files/s]\u001b[A\nCopying files: 4683 files [00:18, 696.75 files/s]\u001b[A\nCopying files: 4753 files [00:18, 696.72 files/s]\u001b[A\nCopying files: 4823 files [00:18, 697.40 files/s]\u001b[A\nCopying files: 4893 files [00:19, 682.30 files/s]\u001b[A\nCopying files: 4966 files [00:19, 695.80 files/s]\u001b[A\nCopying files: 5036 files [00:19, 692.49 files/s]\u001b[A\nCopying files: 5106 files [00:24, 41.01 files/s] \u001b[A\nCopying files: 5166 files [00:24, 54.56 files/s]\u001b[A\nCopying files: 5218 files [00:25, 69.78 files/s]\u001b[A\nCopying files: 5280 files [00:25, 94.74 files/s]\u001b[A\nCopying files: 5340 files [00:25, 125.45 files/s]\u001b[A\nCopying files: 5407 files [00:25, 169.01 files/s]\u001b[A\nCopying files: 5480 files [00:25, 226.33 files/s]\u001b[A\nCopying files: 5546 files [00:25, 281.71 files/s]\u001b[A\nCopying files: 5613 files [00:25, 341.35 files/s]\u001b[A\nCopying files: 5678 files [00:25, 394.99 files/s]\u001b[A\nCopying files: 5745 files [00:25, 450.51 files/s]\u001b[A\nCopying files: 5817 files [00:25, 511.23 files/s]\u001b[A\nCopying files: 5884 files [00:26, 533.60 files/s]\u001b[A\nCopying files: 5955 files [00:26, 577.48 files/s]\u001b[A\nCopying files: 6022 files [00:26, 591.00 files/s]\u001b[A\nCopying files: 6088 files [00:26, 581.16 files/s]\u001b[A\nCopying files: 6163 files [00:26, 624.89 files/s]\u001b[A\nCopying files: 6230 files [00:26, 604.21 files/s]\u001b[A\nCopying files: 6293 files [00:26, 607.54 files/s]\u001b[A\nCopying files: 6361 files [00:26, 624.00 files/s]\u001b[A\nCopying files: 6425 files [00:26, 612.29 files/s]\u001b[A\nCopying files: 6489 files [00:27, 618.25 files/s]\u001b[A\nCopying files: 6565 files [00:27, 657.34 files/s]\u001b[A\nCopying files: 6632 files [00:27, 655.51 files/s]\u001b[A\nCopying files: 6705 files [00:27, 673.60 files/s]\u001b[A\nCopying files: 6773 files [00:27, 673.03 files/s]\u001b[A\nCopying files: 6841 files [00:27, 613.59 files/s]\u001b[A\nCopying files: 6904 files [00:27, 581.82 files/s]\u001b[A\nCopying files: 6977 files [00:27, 621.40 files/s]\u001b[A\nCopying files: 7041 files [00:27, 619.84 files/s]\u001b[A\nCopying files: 7104 files [00:27, 620.62 files/s]\u001b[A\nCopying files: 7174 files [00:28, 642.30 files/s]\u001b[A\nCopying files: 7244 files [00:28, 658.16 files/s]\u001b[A\nCopying files: 7318 files [00:28, 679.80 files/s]\u001b[A\nCopying files: 7387 files [00:28, 663.22 files/s]\u001b[A\nCopying files: 7454 files [00:28, 661.84 files/s]\u001b[A\nCopying files: 7575 files [00:28, 264.30 files/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Training dataset size: 6060\nValidation dataset size: 756\nClass mapping: {'Fire': 0, 'Normal': 1, 'Smoke': 2}\nTest dataset size: 759\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training dataset size: 6060\nValidation dataset size: 756\nClass mapping: {'Fire': 0, 'Normal': 1, 'Smoke': 2}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['val_loss', 'val_acc'])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a67e546e8aba4223a12b92e50f77c953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['lr-AdamW/pg1', 'lr-AdamW/pg2', 'val_loss', 'val_acc'])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['lr-AdamW/pg1', 'lr-AdamW/pg2', 'val_loss', 'val_acc', 'train_loss', 'train_acc'])\nEpoch metrics - Train Loss: 0.1057, Train Acc: 0.9701, Val Loss: 0.0168, Val Acc: 0.9947\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['lr-AdamW/pg1', 'lr-AdamW/pg2', 'val_loss', 'val_acc', 'train_loss', 'train_acc'])\nEpoch metrics - Train Loss: 0.0244, Train Acc: 0.9926, Val Loss: 0.0076, Val Acc: 0.9974\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['lr-AdamW/pg1', 'lr-AdamW/pg2', 'val_loss', 'val_acc', 'train_loss', 'train_acc'])\nEpoch metrics - Train Loss: 0.0142, Train Acc: 0.9959, Val Loss: 0.0068, Val Acc: 0.9960\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['lr-AdamW/pg1', 'lr-AdamW/pg2', 'val_loss', 'val_acc', 'train_loss', 'train_acc'])\nEpoch metrics - Train Loss: 0.0105, Train Acc: 0.9969, Val Loss: 0.0090, Val Acc: 0.9974\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['lr-AdamW/pg1', 'lr-AdamW/pg2', 'val_loss', 'val_acc', 'train_loss', 'train_acc'])\nEpoch metrics - Train Loss: 0.0070, Train Acc: 0.9970, Val Loss: 0.0071, Val Acc: 0.9960\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Metric keys available: dict_keys(['lr-AdamW/pg1', 'lr-AdamW/pg2', 'val_loss', 'val_acc', 'train_loss', 'train_acc'])\nEpoch metrics - Train Loss: 0.0045, Train Acc: 0.9983, Val Loss: 0.0073, Val Acc: 0.9974\nLengths - train_losses: 6, train_accuracies: 6, val_losses: 6, val_accuracies: 6\nTraining metrics plot saved to: /kaggle/working/training_metrics.png\nTraining Metrics:\nEpoch 1: Train Loss: 0.1057, Train Acc: 0.9701, Val Loss: 0.0168, Val Acc: 0.9947\nEpoch 2: Train Loss: 0.0244, Train Acc: 0.9926, Val Loss: 0.0076, Val Acc: 0.9974\nEpoch 3: Train Loss: 0.0142, Train Acc: 0.9959, Val Loss: 0.0068, Val Acc: 0.9960\nEpoch 4: Train Loss: 0.0105, Train Acc: 0.9969, Val Loss: 0.0090, Val Acc: 0.9974\nEpoch 5: Train Loss: 0.0070, Train Acc: 0.9970, Val Loss: 0.0071, Val Acc: 0.9960\nEpoch 6: Train Loss: 0.0045, Train Acc: 0.9983, Val Loss: 0.0073, Val Acc: 0.9974\nTest dataset size: 759\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2df657e58b14adb8a6f4597e814d656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9960474371910095    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.009069639258086681   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9960474371910095     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.009069639258086681    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Test accuracy: 0.9960\nClassification Report:\n              precision    recall  f1-score   support\n\n        Fire       1.00      0.99      0.99       253\n      Normal       0.99      1.00      0.99       253\n       Smoke       1.00      1.00      1.00       253\n\n    accuracy                           1.00       759\n   macro avg       1.00      1.00      1.00       759\nweighted avg       1.00      1.00      1.00       759\n\nFinal test accuracy: 0.9960\nConfusion matrix saved to: /kaggle/working/confusion_matrix.png\nSample predictions saved to: /kaggle/working/sample_predictions.png\nClassification report saved to: /kaggle/working/classification_report.txt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 0 Axes>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}